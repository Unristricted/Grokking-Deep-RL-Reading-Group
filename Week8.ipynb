{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#Actions: \n",
    "# 0: Left\n",
    "# 1: Down\n",
    "# 2: Right\n",
    "# 3: Up\n",
    "# (Prob, State, Reward, isTerminal)\n",
    "MDP = {\n",
    "    0: {\n",
    "        0: [(0.6666666666666666, 0, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 0, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False),\n",
    "            (0.3333333333333333, 0, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 1, 0.0, False),\n",
    "            (0.6666666666666666, 0, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    1: {\n",
    "        0: [(0.3333333333333333, 1, 0.0, False),\n",
    "            (0.3333333333333333, 0, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 0, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 2, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False),\n",
    "            (0.3333333333333333, 0, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    2: {\n",
    "        0: [(0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False),\n",
    "            (0.3333333333333333, 6, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 1, 0.0, False),\n",
    "            (0.3333333333333333, 6, 0.0, False),\n",
    "            (0.3333333333333333, 3, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 6, 0.0, False),\n",
    "            (0.3333333333333333, 3, 0.0, False),\n",
    "            (0.3333333333333333, 2, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 3, 0.0, False),\n",
    "            (0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 1, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        0: [(0.3333333333333333, 3, 0.0, False),\n",
    "            (0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 7, 0.0, True)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 7, 0.0, True),\n",
    "            (0.3333333333333333, 3, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 7, 0.0, True),\n",
    "            (0.6666666666666666, 3, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.6666666666666666, 3, 0.0, False),\n",
    "            (0.3333333333333333, 2, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    4: {\n",
    "        0: [(0.3333333333333333, 0, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 8, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 0, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 0, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    5: {\n",
    "        0: [(1.0, 5, 0, True)],\n",
    "        1: [(1.0, 5, 0, True)],\n",
    "        2: [(1.0, 5, 0, True)],\n",
    "        3: [(1.0, 5, 0, True)]\n",
    "    },\n",
    "    6: {\n",
    "        0: [(0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 10, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 7, 0.0, True)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 7, 0.0, True),\n",
    "            (0.3333333333333333, 2, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 7, 0.0, True),\n",
    "            (0.3333333333333333, 2, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True)\n",
    "        ]\n",
    "    },\n",
    "    7: {\n",
    "        0: [(1.0, 7, 0, True)],\n",
    "        1: [(1.0, 7, 0, True)],\n",
    "        2: [(1.0, 7, 0, True)],\n",
    "        3: [(1.0, 7, 0, True)]\n",
    "    },\n",
    "    8: {\n",
    "        0: [(0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 12, 0.0, True)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 12, 0.0, True),\n",
    "            (0.3333333333333333, 9, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 12, 0.0, True),\n",
    "            (0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 4, 0.0, False),\n",
    "            (0.3333333333333333, 8, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    9: {\n",
    "        0: [(0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 13, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 8, 0.0, False),\n",
    "            (0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 10, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 5, 0.0, True),\n",
    "            (0.3333333333333333, 8, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    10: {\n",
    "        0: [(0.3333333333333333, 6, 0.0, False),\n",
    "            (0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 11, 0.0, True)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 11, 0.0, True),\n",
    "            (0.3333333333333333, 6, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 11, 0.0, True),\n",
    "            (0.3333333333333333, 6, 0.0, False),\n",
    "            (0.3333333333333333, 9, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    11: {\n",
    "        0: [(1.0, 11, 0, True)],\n",
    "        1: [(1.0, 11, 0, True)],\n",
    "        2: [(1.0, 11, 0, True)],\n",
    "        3: [(1.0, 11, 0, True)]\n",
    "    },\n",
    "    12: {\n",
    "        0: [(1.0, 12, 0, True)],\n",
    "        1: [(1.0, 12, 0, True)],\n",
    "        2: [(1.0, 12, 0, True)],\n",
    "        3: [(1.0, 12, 0, True)]\n",
    "    },\n",
    "    13: {\n",
    "        0: [(0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 12, 0.0, True),\n",
    "            (0.3333333333333333, 13, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 12, 0.0, True),\n",
    "            (0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 9, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 9, 0.0, False),\n",
    "            (0.3333333333333333, 12, 0.0, True)\n",
    "        ]\n",
    "    },\n",
    "    14: {\n",
    "        0: [(0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False)\n",
    "        ],\n",
    "        1: [(0.3333333333333333, 13, 0.0, False),\n",
    "            (0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 15, 1.0, True)\n",
    "        ],\n",
    "        2: [(0.3333333333333333, 14, 0.0, False),\n",
    "            (0.3333333333333333, 15, 1.0, True),\n",
    "            (0.3333333333333333, 10, 0.0, False)\n",
    "        ],\n",
    "        3: [(0.3333333333333333, 15, 1.0, True),\n",
    "            (0.3333333333333333, 10, 0.0, False),\n",
    "            (0.3333333333333333, 13, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    15: {\n",
    "        0: [(1.0, 15, 0, True)],\n",
    "        1: [(1.0, 15, 0, True)],\n",
    "        2: [(1.0, 15, 0, True)],\n",
    "        3: [(1.0, 15, 0, True)]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration(P, gamma=1.0, theta=1e-10):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm to compute the optimal state-value function and policy.\n",
    "    \n",
    "    Parameters:\n",
    "    P : list\n",
    "        Transition probability matrix for the MDP.\n",
    "        P[s][a] is a list of (probability, next_state, reward, done) tuples.\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "    theta : float\n",
    "        Convergence threshold.\n",
    "    \n",
    "    Returns:\n",
    "    V : np.ndarray\n",
    "        Optimal state-value function.\n",
    "    pi : np.ndarray\n",
    "        Optimal policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(len(P), dtype=np.float64)  # (1) Initialize state-value function\n",
    "\n",
    "    while True:  # (2) Loop until convergence\n",
    "        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)  # (3) Initialize action-value function/Q function\n",
    "\n",
    "        for s in range(len(P)):  # (4) Iterate over all states\n",
    "            for a in range(len(P[s])):  # Iterate over all actions for each state\n",
    "                for prob, next_state, reward, done in P[s][a]:  # (5) Iterate over transitions\n",
    "                    # (6) Calculate the Q-value for each action\n",
    "                    #A version of the Bellman equation\n",
    "                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        \n",
    "        # (7) Check if the state-value function has converged\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            break\n",
    "        \n",
    "        # (8) Update the state-value function\n",
    "        V = np.max(Q, axis=1)\n",
    "        # (9) Extract the optimal policy\n",
    "        pi = np.argmax(Q, axis=1)   \n",
    "\n",
    "\n",
    "    return V, pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n",
      "[0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "V, pi = value_iteration(MDP)\n",
    "\n",
    "#state value function\n",
    "print(V )\n",
    "#Optimal policy / best action to take according to the state value function\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??\n",
    "import gym\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# run(15000)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(episodes, is_training, render)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q[state,:])\n\u001b[0;32m---> 36\u001b[0m new_state,reward,terminated,truncated,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m     39\u001b[0m     q[state,action] \u001b[38;5;241m=\u001b[39m q[state,action] \u001b[38;5;241m+\u001b[39m learning_rate_a \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     40\u001b[0m         reward \u001b[38;5;241m+\u001b[39m discount_factor_g \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q[new_state,:]) \u001b[38;5;241m-\u001b[39m q[state,action]\n\u001b[1;32m     41\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:308\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    436\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def run(episodes, is_training=True, render=False):\n",
    "\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True, render_mode='human' if render else None)\n",
    "\n",
    "    if(is_training):\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n)) # init a 64 x 4 array\n",
    "    else:\n",
    "        f = open('frozen_lake8x8.pkl', 'rb')\n",
    "        q = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    learning_rate_a = 0.9 # alpha or learning rate\n",
    "    discount_factor_g = 0.9 # gamma or discount rate. Near 0: more weight/reward placed on immediate state. Near 1: more on future state.\n",
    "    epsilon = 1         # 1 = 100% random actions\n",
    "    epsilon_decay_rate = 0.0001        # epsilon decay rate. 1/0.0001 = 10,000\n",
    "    rng = np.random.default_rng()   # random number generator\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]  # states: 0 to 63, 0=top left corner,63=bottom right corner\n",
    "        terminated = False      # True when fall in hole or reached goal\n",
    "        truncated = False       # True when actions > 200\n",
    "\n",
    "        while(not terminated and not truncated):\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "            else:\n",
    "                action = np.argmax(q[state,:])\n",
    "\n",
    "            new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                q[state,action] = q[state,action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state,:]) - q[state,action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        if(epsilon==0):\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t-100):(t+1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.savefig('frozen_lake8x8.png')\n",
    "\n",
    "    if is_training:\n",
    "        f = open(\"frozen_lake8x8.pkl\",\"wb\")\n",
    "        pickle.dump(q, f)\n",
    "        f.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run(15000)\n",
    "\n",
    "    run(1000, is_training=True, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m             pickle\u001b[38;5;241m.\u001b[39mdump(q, f)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Run the training for 1000 episodes\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(episodes, is_training, render)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q[state, :])  \u001b[38;5;66;03m# Best action based on Q-table\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Update Q-value using the Q-learning formula\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     q[state, action] \u001b[38;5;241m=\u001b[39m q[state, action] \u001b[38;5;241m+\u001b[39m learning_rate_a \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     44\u001b[0m         reward \u001b[38;5;241m+\u001b[39m discount_factor_g \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q[new_state, :]) \u001b[38;5;241m-\u001b[39m q[state, action]\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:308\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    436\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "#4X4\n",
    "\n",
    "def run(episodes, is_training=True, render=False):\n",
    "    # Create a 4x4 Frozen Lake environment\n",
    "    env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False, render_mode='human' if render else None)\n",
    "\n",
    "    # Initialize the Q-table for the 4x4 grid\n",
    "    if is_training:\n",
    "        q = np.zeros((env.observation_space.n, env.action_space.n))  # init a 16 x 4 array for 4x4\n",
    "    else:\n",
    "        with open('frozen_lake4x4.pkl', 'rb') as f:\n",
    "            q = pickle.load(f)\n",
    "\n",
    "    learning_rate_a = 0.9  # Alpha or learning rate\n",
    "    discount_factor_g = 0.9  # Gamma or discount rate\n",
    "    epsilon = 1  # 1 = 100% random actions\n",
    "    epsilon_decay_rate = 0.0001  # Epsilon decay rate\n",
    "    rng = np.random.default_rng()  # Random number generator\n",
    "\n",
    "    rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()[0]  # Reset environment and get initial state\n",
    "        terminated = False  # True when fall in hole or reached goal\n",
    "        truncated = False  # True when actions exceed a threshold\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            if is_training and rng.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Random action\n",
    "            else:\n",
    "                action = np.argmax(q[state, :])  # Best action based on Q-table\n",
    "\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if is_training:\n",
    "                # Update Q-value using the Q-learning formula\n",
    "                q[state, action] = q[state, action] + learning_rate_a * (\n",
    "                    reward + discount_factor_g * np.max(q[new_state, :]) - q[state, action]\n",
    "                )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        # Epsilon decay for exploration-exploitation balance\n",
    "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
    "\n",
    "        # Reduce learning rate if epsilon is zero\n",
    "        if epsilon == 0:\n",
    "            learning_rate_a = 0.0001\n",
    "\n",
    "        # Store rewards for plotting\n",
    "        if reward == 1:\n",
    "            rewards_per_episode[i] = 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Plotting the rewards over episodes\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for t in range(episodes):\n",
    "        sum_rewards[t] = np.sum(rewards_per_episode[max(0, t - 100):(t + 1)])\n",
    "    plt.plot(sum_rewards)\n",
    "    plt.savefig('frozen_lake4x4.png')\n",
    "\n",
    "    # Save Q-table if training\n",
    "    if is_training:\n",
    "        with open(\"frozen_lake4x4.pkl\", \"wb\") as f:\n",
    "            pickle.dump(q, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Run the training for 1000 episodes\n",
    "    run(1000, is_training=True, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

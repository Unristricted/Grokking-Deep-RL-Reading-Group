{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d9adf0-1ca2-47cb-83cc-1f0d5e369e25",
   "metadata": {},
   "source": [
    "#### Similar to how environments interact with instances of `tensordict.TensorDict`, the modules used to represent policies and value functions also do the same. The core idea is simple: encapsulate a standard `torch.nn.Module` (or any other function) within a class that knows which entries need to be read and passed to the module, and then records the results with the assigned entries. To illustrate this, we will use the simplest policy possible: a deterministic map from the observation space to the action space. For maximum generality, we will use a `torch.nn.LazyLinear` module with the Pendulum environment we instantiated in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff0a33d-2e85-43f2-b468-7983ff1aa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchrl.envs import GymEnv, set_gym_backend\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "with set_gym_backend(\"gym\"):\n",
    "    env = GymEnv(\"Pendulum-v1\")\n",
    "\n",
    "module = torch.nn.LazyLinear(out_features=env.action_spec.shape[-1])\n",
    "policy = TensorDictModule(\n",
    "    module,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190eb30-b425-484c-9d6c-bc16d081e230",
   "metadata": {},
   "source": [
    "#### This is all that's required to execute our policy! The use of a lazy module allows us to bypass the need to fetch the shape of the observation space, as the module will automatically determine it. This policy is now ready to be run in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f606d0b-2951-4729-a90f-c724bbf3189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f955e-4581-4a7b-9749-4025446c90a9",
   "metadata": {},
   "source": [
    "## Specialized wrappers\n",
    "\n",
    "To simplify the incorporation of `torch.nn.Module`s into your codebase, TorchRL offers a range of specialized wrappers designed to be used as actors, including:\n",
    "- `torchrl.modules.tensordict_module.Actor`\n",
    "- `torchrl.modules.tensordict_module.ProbabilisticActor`\n",
    "- `torchrl.modules.tensordict_module.ActorValueOperator`\n",
    "- `torchrl.modules.tensordict_module.ActorCriticOperator`\n",
    "\n",
    "For example, `torchrl.modules.tensordict_module.Actor` provides default values for the `in_keys` and `out_keys`, making integration with many common environments straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8510db-073d-4aa5-9bf0-26ff01194a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.modules import Actor\n",
    "\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082a63f-df36-4101-89f7-cfa8b3e4d96d",
   "metadata": {},
   "source": [
    "# Networks\n",
    "\n",
    "#### TorchRL also provides regular modules that can be used without recurring to tensordict features. The two most common networks you will encounter are the `torchrl.modules.MLP` and the `torchrl.modules.ConvNet` (CNN) modules. We can substitute our policy module with one of these:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708bd75a-e2d3-47d8-a95c-987ba6abcf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import MLP\n",
    "\n",
    "module = MLP(\n",
    "    out_features=env.action_spec.shape[-1],\n",
    "    num_cells=[32, 64],\n",
    ")\n",
    "policy = Actor(module)\n",
    "rollout = env.rollout(max_steps=10, policy=policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a9c4a-a828-4945-84a5-7ee5bf5048d8",
   "metadata": {},
   "source": [
    "#### You can control the sampling of the action to use the expected value or other properties of the distribution instead of using random samples if your application requires it. This can be controlled via the `set_exploration_type()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0a09a3-7c1c-4374-98ec-b54f2fab81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "    # takes the mean as action\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Samples actions according to the dist\n",
    "    rollout = env.rollout(max_steps=10, policy=policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362263d-3580-4865-9ec0-1367ce646777",
   "metadata": {},
   "source": [
    "#### Stochastic policies like this somewhat naturally trade off exploration and exploitation, but deterministic policies won’t. Fortunately, TorchRL can also palliate to this with its exploration modules. We will take the example of the EGreedyModule exploration module. To see this module in action, let’s revert to a deterministic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78a61ac-c5a3-4d83-81db-ca46c87bd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict.nn import TensorDictSequential\n",
    "from torchrl.modules import EGreedyModule\n",
    "\n",
    "policy = Actor(MLP(3, 1, num_cells=[32, 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f32dd-93fe-4982-9539-96f03a7d2731",
   "metadata": {},
   "source": [
    "#### Our `ε`-greedy exploration module will usually be customized with a number of annealing frames and an initial value for the `ε` parameter. A value of `ε = 1` means that every action taken is random, while `ε=0` means that there is no exploration at all. To anneal (i.e., decrease) the exploration factor, a call to `torchrl.modules.EGreedyModule.step` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a270ffa2-03d3-4d52-ad16-822b8a7d5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_module = EGreedyModule(\n",
    "    spec=env.action_spec, annealing_num_steps=1000, eps_init=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e93c4-414b-4242-bab6-0f0b2e38805e",
   "metadata": {},
   "source": [
    "#### To build our explorative policy, we only had to concatenate the deterministic policy module with the exploration module within a TensorDictSequential module (which is the analogous to Sequential in the tensordict realm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8be4f52-8b7b-468a-98ec-334bcc2ba941",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policy = TensorDictSequential(policy, exploration_module)\n",
    "\n",
    "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
    "    # Turns off exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    # Turns on exploration\n",
    "    rollout = env.rollout(max_steps=10, policy=exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e1d4f-cbc7-46de-8373-d6a03cb77d8e",
   "metadata": {},
   "source": [
    "# Q-Value actors\n",
    "#### In some settings, the policy isn’t a standalone module but is constructed on top of another module. This is the case with Q-Value actors. In short, these actors require an estimate of the action value (most of the time discrete) and will greedily pick up the action with the highest value. In some settings (finite discrete action space and finite discrete state space), one can just store a 2D table of state-action pairs and pick up the action with the highest value. The innovation brought by DQN was to scale this up to continuous state spaces by utilizing a neural network to encode for the `Q(s, a)` value map. Let’s consider another environment with a discrete action space for a clearer understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a0dc893-e0aa-4f6b-b7f7-a16e5f42379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHot(\n",
      "    shape=torch.Size([2]),\n",
      "    space=CategoricalBox(n=2),\n",
      "    device=cpu,\n",
      "    dtype=torch.int64,\n",
      "    domain=discrete)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "with set_gym_backend(\"gym\"):\n",
    "    env = GymEnv(\"CartPole-v1\")\n",
    "print(env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cd9ee-cc83-41fe-9064-67f440157a27",
   "metadata": {},
   "source": [
    "#### We build a value network that produces one value per action when it reads a state from the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7aacba84-7f5e-44d8-ab9d-7679c135ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "value_net = TensorDictModule(\n",
    "    MLP(out_features=num_actions, num_cells=[32, 32]),\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action_value\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10036c82-c33a-44e5-9624-f0c1e4e347eb",
   "metadata": {},
   "source": [
    "#### We can easily build our Q-Value actor by adding a `QValueModule` after our value network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "411471f6-2847-45d4-9b7a-89490d2517ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import QValueModule\n",
    "policy = TensorDictSequential(\n",
    "    value_net,  # writes action values in our tensordict\n",
    "    QValueModule(spec=env.action_spec),  # Reads the \"action_value\" entry by default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74d2ad-6ede-4aaa-97e9-4b9285245dac",
   "metadata": {},
   "source": [
    "#### Let’s check it out! We run the policy for a couple of steps and look at the output. We should find an `\"action_value`\" as well as a `\"chosen_action_value\"` entries in the rollout that we obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0ca853b-2396-4a68-830c-268d22ba8c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_value: Tensor(shape=torch.Size([3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        chosen_action_value: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=3, policy=policy)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b487d3-830b-4588-8442-954b6c7ed0f7",
   "metadata": {},
   "source": [
    "#### Because it relies on the argmax operator, this policy is deterministic. During data collection, we will need to explore the environment. For that, we are using the `EGreedyModule`once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0b10a9f-6150-4130-a235-4717c3b2af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([3]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([3, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([3]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "policy_explore = TensorDictSequential(policy, EGreedyModule(env.action_spec))\n",
    "with set_exploration_type(ExplorationType.RANDOM):\n",
    "    rollout_explore = env.rollout(max_steps=3, policy=policy_explore)\n",
    "\n",
    "print(rollout_explore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

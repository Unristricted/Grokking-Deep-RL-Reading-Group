{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02bea14-ee21-4737-a5d7-9fc129fe0c26",
   "metadata": {},
   "source": [
    "## Creating an environment\n",
    "\n",
    "In essence, TorchRL does not directly provide environments, but instead offers wrappers for other libraries that encapsulate the simulators. The `torchrl.envs` module can be viewed as a provider for a generic environment API, as well as a central hub for simulation backends like gym, which is what we'll use. Creating your environment is typically as straightforward as the underlying backend API allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e69414-dd78-436a-8b43-63d83d7b07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import GymEnv, set_gym_backend\n",
    "with set_gym_backend(\"gym\"):\n",
    "    env = GymEnv(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438df40d-17bb-4923-a572-97c028143e5d",
   "metadata": {},
   "source": [
    "## Running an environment\n",
    "\n",
    "Environments in TorchRL have two crucial methods: `torchrl.envs.EnvBase.reset`, which initiates\n",
    "an episode, and `torchrl.envs.EnvBase.step`, which executes an action selected by the actor.\n",
    " \n",
    "In TorchRL, environment methods read and write `tensordict.TensorDict` instances. Essentially, `tensordict.TensorDict` is a generic key-based data carrier for tensors.\n",
    "\n",
    "The benefit of using TensorDicts is that they enable us to handle simple and complex data structures interchangeably. As our function signatures are very generic, they eliminate the challenge of accommodating different data formats. In simpler terms, they allow us to operate on both simple and highly complex environments, since their user-facing API is identical and simple!\n",
    "\n",
    "Let's put the environment into action and see what a tensordict instance looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d31dc36-ad7e-4b22-9ff7-c70f1a11c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset = env.reset()\n",
    "print(reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d438a-a3e4-4f6c-93ff-2c54f4b49ab8",
   "metadata": {},
   "source": [
    "### Now let's take a random action in the action space. First, sample the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd766694-0ae9-4db0-b92b-9a0171f8b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "reset_with_action = env.rand_action(reset)\n",
    "print(reset_with_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a91c7-8e0c-42ee-b746-d64adf6c8abc",
   "metadata": {},
   "source": [
    "### This tensordict has the same structure as the one obtained from `torchrl.envs.EnvBase` with an additional `\"action\"` entry. You can access the action easily, like you would do with a regular dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1234d81-dfd1-4d77-a779-9f7e45bdad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7638])\n"
     ]
    }
   ],
   "source": [
    "print(reset_with_action[\"action\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dccbd5-56d0-43ea-8098-686fe55e0862",
   "metadata": {},
   "source": [
    "### We now need to pass this action to the environment. We'll be passing the entire tensordict to the ``step`` method, since there might be more than one tensor to be read in more advanced cases like Multi-Agent RL or stateless environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c2f6192-f1f2-45c2-87d2-58241d50d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "stepped_data = env.step(reset_with_action)\n",
    "print(stepped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12739a92-5970-4104-95f3-2b9696b7844c",
   "metadata": {},
   "source": [
    "### The last bit of information you need to run a rollout in the environment is how to bring that ``\"next\"`` entry at the root to perform the next step. TorchRL provides a dedicated `torchrl.envs.utils.step_mdp` function that does just that: it filters out the information you won't need and delivers a data structure corresponding to your observation after a step in the Markov Decision Process, or MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c470f070-6661-49cc-84ba-0aeb897df4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import step_mdp\n",
    "\n",
    "data = step_mdp(stepped_data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cf761-c92c-494a-b51d-e6acd9c77f78",
   "metadata": {},
   "source": [
    "### Writing down those three steps (computing an action, making a step, moving in the MDP) can be a bit tedious and repetitive. Fortunately, TorchRL provides a nice `torchrl.envs.EnvBase.rollout` function that allows you to run them in a closed loop at will:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc67671-3afa-42b8-9dd5-6c3344cd3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = env.rollout(max_steps=10)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e797b15-ea28-4ff8-8be5-4de08890ccf6",
   "metadata": {},
   "source": [
    "### This data looks pretty much like the ``stepped_data`` above with the exception of its batch-size, which now equates the number of steps we provided through the ``max_steps`` argument. The magic of tensordict doesn't end there: if you're interested in a single transition of this environment, you can index the tensordict like you would index a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33a29dcc-6c82-44f3-b209-3ac3180e09d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "transition = rollout[3]\n",
    "print(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c6735-22e2-4d8e-af79-b5420924c1aa",
   "metadata": {},
   "source": [
    "### Most of the time, you'll want to modify the output of the environment to better suit your requirements. For example, you might want to monitor the number of steps executed since the last reset, resize images, or stack consecutive observations together. In this section, we'll examine a simple transform, the `torchrl.envs.transforms.StepCounter` transform. The transform is integrated with the environment through a `torchrl.envs.transforms.TransformedEnv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1139d6c-8fa9-404c-ade9-82dec835c683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        step_count: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import StepCounter, TransformedEnv\n",
    "\n",
    "transformed_env = TransformedEnv(env, StepCounter(max_steps=10))\n",
    "rollout = transformed_env.rollout(max_steps=100)\n",
    "print(rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db743187-a2fb-480b-b57b-233734caacbd",
   "metadata": {},
   "source": [
    "### As you can see, our environment now has one more entry, `\"step_count\"` that tracks the number of steps since the last reset. Given that we passed the optional argument `max_steps=10` to the transform constructor, we also truncated the trajectory after 10 steps (not completing a full rollout of 100 steps like we asked with the `rollout` call). We can see that the trajectory was truncated by looking at the truncated entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d81180e-0529-4bd9-bdc1-b719653ded01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "print(rollout[\"next\", \"truncated\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
